Session
	-takes as input io and transport layer'
	-is driven by on_message and on_close events from the transport layer
Client
Server
in creating the tests, you can access reconnect class individually. in design, it hid under the reconnect_glue class.
	-you can instantiate instance of reconnect class and in doing so pass it a tOps and rS of your liking and design
in the original application,it seems you create an instance of reconnect glue but then access rS and tie other layers to rS
reconnect_glue
	has a transport layer
	it sets up callbacks dealing with the transport layer
	then it creates an instance of reconnect and passes it the callbacks

ReconnectSignals
	-what is the purpose of them?
	-if they are a way for other layers to be notified by reconnect about events that have happened, then we need to better hook them up

ReconnectGlue
	-is the class that needs to interact with all other layers

Reconnect
	-its a class that implements reconnection policies revolving around the transport layer
	-the layers above should rely on the signals coming out of this class to determine when
	the client is connected or disconnected and accordingly send/receive data

To test Reconnect class, we can always create dummy tOps, rS and verify the reconnect logic implementation from there.

ReconnectGlue
	-needs to export rS further other to classes using ReconnectGlue
	-ReconnectGlue needs to implement callbacks that are mirrors of callbacks from ReconnectController
	-ReconnectGlue needs to implement actuation methods that are mirrors of actuation methods in ReconnectController

Client
	-application
		it needs to use ReconnectGlue to actuate connection(via accessing the Reconnect instance) and
		rely on exported rS on connection-related events(what events should the client be interested in?)
		and propagate these to the relevant layers(session:once connected, it should be notified so it can start doing
		boot notifications)
	-tests
	-r.w
		Session has access to transport layer for sending packets out
		But it needs to be notified by the application that the transport layer is connected because that happens through the reconnect module(what if we don't want to use the reconnect module?)


the Reconnect module 
	-needs to yes implement the reconnect policy
	-needs to be notified on events happening in the transport layer
	-needs to be notified of relevant events happening in other layers
	-needs to notify other layers of different events related to the connection

	struct ReconnectSignals {
	  std::function<void()> on_connecting;
	  std::function<void()> on_connected;
	  std::function<void(CloseReason)> on_closed;
	  std::function<void()> on_online;
	  std::function<void()> on_offline;
	  std::function<void(std::chrono::milliseconds)> on_backoff_scheduled;// Notifies observers when a backoff delay is computed and scheduled. Receives the delay (ms). Called after compute_backoff_ and posting the timer; handler should be quick/non‑blocking.
	};

* **Metrics:** counters for reconnect attempts, last reconnect duration, last disconnect reason—this feeds Milestone 6. 

When Session sends BootNotification, it should deal with Pending/Cancelled states?


//to do for multi-threaded design hopefully once entire application is completed
Note (brief): strand vs multi-threaded io_context::run()
If you run the same io_context on 2+ threads, Asio may execute different async handlers concurrently.
A strand is a serialization tool: any handlers post()ed/dispatch()ed through the same strand won’t run at the same time, even with multiple io.run() threads.
Use a strand for stateful modules (ReconnectController / Session) to avoid races on flags, timers, maps.
In this codebase

WsClient already uses strand_ to serialize writes (send()/do_write()).
Read/close/connect callbacks (do_read(), on_connected_, on_closed_) may still call into higher layers; if io is multi-threaded, route those calls onto a controller/session strand in the glue layer.
Why run io on multiple threads

Higher throughput / less latency when there are multiple connections or heavy handlers (e.g., JSON parsing, logging), so one slow handler doesn’t block all others.
Rule of thumb

Single io.run() thread: simplest.
Multi-threaded io.run(): enforce “all controller/session entrypoints run on a strand” (or add locking everywhere).

if( f.wait_for() != std::future_status::ready )
{
	return;
}

you are changing the implementation of on_connected and on_closed so that they now run on a promise.

a plain time_point would need a fake default:
	steady_clock::time_point{}

would force a made-up default like CloseReason::Clean which is wrong if nothing has ever disconnected

can they be normal types instead?
	yes, if you're ok with sentinel semantics
is the value meaningful from construction time?


#linkedin post#
Been deepening my C++ skills by building an async WebSocket transport using Boost.Asio/Beast.
Focused a lot on concurrency correctness: strands, handler execution context, and avoiding races in multi-threaded io_context::run().
Worked through real reliability problems: reconnect state machines, backoff/jitter policies, and “no double send” guarantees on disconnects.
Explored API design tradeoffs: callbacks + timers vs promise/future wrappers for connection lifecycle and in-flight requests.
Also added practical test hooks/telemetry: forced disconnect paths, event timestamps, and reconnect metrics for diagnostics.
#linkedin post#

#compilation hotspots
	-move boost/beast.hpp and boost/beast/websocket.hpp out of headers into .cpp
	-use forward decl. + PIMPL for members that require Beast/WebSocket
		-how?
	-stop reincluding stuff
		-ocpp_model.hpp
		-test_server.hpp
		-ws_client.hpp
		-session.hpp
		-reconnect_glue.hpp
	-keep data structs in a light header
	-move to_json/from_json, parse_frame etc. into a .cpp file
	-mvoe to objects compiled once and link each test binary against those objects


#why impl was suggested to have unique_ptr in this implementation? what assumptions did you have about reconnect
#and wsclient for this suggestion?

#pragma once

#include <cstdint>
#include <functional>
#include <memory>
#include <string>

namespace boost::asio { class io_context; }

class ClientLoop {
public:
    enum class State : std::uint8_t {
        Offline,
        Connecting,
        Online,
    };

    struct Config {
        std::string host = "127.0.0.1";
        unsigned short port = 0;

        // Optional knobs you can route into ReconnectController later.
        // Keep Exercise 1 thin: policy can remain in ReconnectController.
    };

    // Factories allow tests to inject fakes and keep "single owner" here.
    struct Factories {
        // Create a new transport instance for each connection attempt.
        std::function<std::unique_ptr<class WsClient>(boost::asio::io_context&)> make_transport;

        // Create a new session for each successful connection.
        std::function<std::unique_ptr<class Session>(boost::asio::io_context&, class WsClient&)> make_session;

        // Optional: called whenever we transition Offline->Online
        std::function<void()> on_online = [] {};
        // Optional: called whenever we transition Online->Offline
        std::function<void()> on_offline = [] {};
    };

    ClientLoop(boost::asio::io_context& ioc, Config cfg, Factories f);
    ~ClientLoop();

    ClientLoop(const ClientLoop&) = delete;
    ClientLoop& operator=(const ClientLoop&) = delete;

    void start();
    void stop();

    // Probes for tests
    State state() const;
    std::uint64_t connect_attempts() const;
    std::uint64_t online_transitions() const;

private:
    struct Impl;
    std::unique_ptr<Impl> impl_;
};



#include "client_loop.hpp"

#include "reconnect.hpp"
#include "session.hpp"
#include "ws_client.hpp"

#include <boost/asio/io_context.hpp>
#include <utility>

struct ClientLoop::Impl {
    boost::asio::io_context& ioc;
    Config cfg;
    Factories f;

    State st = State::Offline;
    std::uint64_t attempts = 0;
    std::uint64_t online_edges = 0;

    // Own the lifecycle objects (resettable across reconnects)
    std::unique_ptr<ReconnectController> rc;
    std::unique_ptr<WsClient> transport;
    std::unique_ptr<Session> session;

    bool started = false;

    Impl(boost::asio::io_context& i, Config c, Factories factories)
        : ioc(i), cfg(std::move(c)), f(std::move(factories)) {}

    void set_state(State s) {
        if (st == s) return;

        const bool was_online = (st == State::Online);
        const bool now_online = (s == State::Online);

        st = s;

        if (!was_online && now_online) {
            ++online_edges;
            f.on_online();
        } else if (was_online && !now_online) {
            f.on_offline();
        }
    }

    void teardown_connection() {
        // Ensure the session sees close so it can fail pending calls, stop timers, etc.
        // TODO(adapt): call whatever "on_close" / "stop" is appropriate in your Session.
        if (session) {
            // session->on_close();
        }
        session.reset();
        transport.reset();
    }

    void wire_transport_callbacks() {
        // Single owner rule: only ClientLoop wires these callbacks, once per transport instance.
        // TODO(adapt): replace with your actual WsClient callback registration API.

        // transport->set_on_open([this] { on_open(); });
        // transport->set_on_close([this](/*...*/) { on_close(); });
        // transport->set_on_message([this](std::string_view msg) { on_message(msg); });
    }

    void connect_now() {
        ++attempts;
        set_state(State::Connecting);

        teardown_connection();

        transport = f.make_transport(ioc);
        wire_transport_callbacks();

        // TODO(adapt): call your transport connect/start method.
        // transport->connect(cfg.host, cfg.port);

        // If your ReconnectController owns dialing instead, then connect_now() should
        // delegate to rc, and rc should call back into on_open/on_close hooks.
    }

    // Called by transport when websocket is open
    void on_open() {
        // Create new Session per connection.
        session = f.make_session(ioc, *transport);

        set_state(State::Online);

        // Exercise 2+ will hook "boot on connect" here (not in tests).
        // e.g. session->start_boot();
    }

    void on_close() {
        teardown_connection();
        set_state(State::Offline);

        // Schedule reconnect via ReconnectController
        // TODO(adapt): create/configure rc and schedule next attempt.
        // rc->schedule([this]{ connect_now(); });
    }

    void on_message(/*std::string_view*/ ) {
        // TODO(adapt): forward message to session decode/router.
        // session->on_message(msg);
    }
};

ClientLoop::ClientLoop(boost::asio::io_context& ioc, Config cfg, Factories f)
    : impl_(std::make_unique<Impl>(ioc, std::move(cfg), std::move(f))) {}

ClientLoop::~ClientLoop() = default;

void ClientLoop::start() {
    if (impl_->started) return;
    impl_->started = true;

    // Keep Exercise 1 minimal: start in Offline, then begin connecting.
    impl_->set_state(State::Offline);
    impl_->connect_now();
}

void ClientLoop::stop() {
    if (!impl_->started) return;
    impl_->started = false;

    impl_->teardown_connection();
    impl_->set_state(State::Offline);

    // TODO(adapt): stop/cancel reconnect controller timers if needed.
    // if (impl_->rc) impl_->rc->stop();
}

ClientLoop::State ClientLoop::state() const { return impl_->st; }
std::uint64_t ClientLoop::connect_attempts() const { return impl_->attempts; }
std::uint64_t ClientLoop::online_transitions() const { return impl_->online_edges; }


#include <gtest/gtest.h>
#include <boost/asio/io_context.hpp>

#include "client_loop.hpp"
#include "ws_client.hpp"
#include "session.hpp"

TEST(ClientLoop, StartsOffline) {
    boost::asio::io_context ioc;

    ClientLoop::Config cfg;
    cfg.host = "127.0.0.1";
    cfg.port = 12345;

    ClientLoop::Factories f;
    f.make_transport = [](boost::asio::io_context& i) {
        // TODO: swap to a fake WsClient if you have one.
        return std::make_unique<WsClient>(i);
    };
    f.make_session = [](boost::asio::io_context& i, WsClient& ws) {
        return std::make_unique<Session>(i, ws);
    };

    ClientLoop loop(ioc, cfg, std::move(f));

    // Before start(): must be Offline and zero transitions.
    EXPECT_EQ(loop.state(), ClientLoop::State::Offline);
    EXPECT_EQ(loop.online_transitions(), 0u);
}