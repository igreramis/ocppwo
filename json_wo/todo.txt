Session
	-takes as input io and transport layer'
	-is driven by on_message and on_close events from the transport layer
Client
Server
in creating the tests, you can access reconnect class individually. in design, it hid under the reconnect_glue class.
	-you can instantiate instance of reconnect class and in doing so pass it a tOps and rS of your liking and design
in the original application,it seems you create an instance of reconnect glue but then access rS and tie other layers to rS
reconnect_glue
	has a transport layer
	it sets up callbacks dealing with the transport layer
	then it creates an instance of reconnect and passes it the callbacks

ReconnectSignals
	-what is the purpose of them?
	-if they are a way for other layers to be notified by reconnect about events that have happened, then we need to better hook them up

ReconnectGlue
	-is the class that needs to interact with all other layers

Reconnect
	-its a class that implements reconnection policies revolving around the transport layer
	-the layers above should rely on the signals coming out of this class to determine when
	the client is connected or disconnected and accordingly send/receive data

To test Reconnect class, we can always create dummy tOps, rS and verify the reconnect logic implementation from there.

ReconnectGlue
	-needs to export rS further other to classes using ReconnectGlue
	-ReconnectGlue needs to implement callbacks that are mirrors of callbacks from ReconnectController
	-ReconnectGlue needs to implement actuation methods that are mirrors of actuation methods in ReconnectController

Client
	-application
		it needs to use ReconnectGlue to actuate connection(via accessing the Reconnect instance) and
		rely on exported rS on connection-related events(what events should the client be interested in?)
		and propagate these to the relevant layers(session:once connected, it should be notified so it can start doing
		boot notifications)
	-tests
	-r.w
		Session has access to transport layer for sending packets out
		But it needs to be notified by the application that the transport layer is connected because that happens through the reconnect module(what if we don't want to use the reconnect module?)


the Reconnect module 
	-needs to yes implement the reconnect policy
	-needs to be notified on events happening in the transport layer
	-needs to be notified of relevant events happening in other layers
	-needs to notify other layers of different events related to the connection

	struct ReconnectSignals {
	  std::function<void()> on_connecting;
	  std::function<void()> on_connected;
	  std::function<void(CloseReason)> on_closed;
	  std::function<void()> on_online;
	  std::function<void()> on_offline;
	  std::function<void(std::chrono::milliseconds)> on_backoff_scheduled;// Notifies observers when a backoff delay is computed and scheduled. Receives the delay (ms). Called after compute_backoff_ and posting the timer; handler should be quick/non‑blocking.
	};

* **Metrics:** counters for reconnect attempts, last reconnect duration, last disconnect reason—this feeds Milestone 6. 

When Session sends BootNotification, it should deal with Pending/Cancelled states?


//to do for multi-threaded design hopefully once entire application is completed
Note (brief): strand vs multi-threaded io_context::run()
If you run the same io_context on 2+ threads, Asio may execute different async handlers concurrently.
A strand is a serialization tool: any handlers post()ed/dispatch()ed through the same strand won’t run at the same time, even with multiple io.run() threads.
Use a strand for stateful modules (ReconnectController / Session) to avoid races on flags, timers, maps.
In this codebase

WsClient already uses strand_ to serialize writes (send()/do_write()).
Read/close/connect callbacks (do_read(), on_connected_, on_closed_) may still call into higher layers; if io is multi-threaded, route those calls onto a controller/session strand in the glue layer.
Why run io on multiple threads

Higher throughput / less latency when there are multiple connections or heavy handlers (e.g., JSON parsing, logging), so one slow handler doesn’t block all others.
Rule of thumb

Single io.run() thread: simplest.
Multi-threaded io.run(): enforce “all controller/session entrypoints run on a strand” (or add locking everywhere).

if( f.wait_for() != std::future_status::ready )
{
	return;
}

you are changing the implementation of on_connected and on_closed so that they now run on a promise.

a plain time_point would need a fake default:
	steady_clock::time_point{}

would force a made-up default like CloseReason::Clean which is wrong if nothing has ever disconnected

can they be normal types instead?
	yes, if you're ok with sentinel semantics
is the value meaningful from construction time?


#linkedin post#
Been deepening my C++ skills by building an async WebSocket transport using Boost.Asio/Beast.
Focused a lot on concurrency correctness: strands, handler execution context, and avoiding races in multi-threaded io_context::run().
Worked through real reliability problems: reconnect state machines, backoff/jitter policies, and “no double send” guarantees on disconnects.
Explored API design tradeoffs: callbacks + timers vs promise/future wrappers for connection lifecycle and in-flight requests.
Also added practical test hooks/telemetry: forced disconnect paths, event timestamps, and reconnect metrics for diagnostics.
#linkedin post#